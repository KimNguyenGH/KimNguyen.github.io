---
title: "Homework 9 - Sentiment Analysis"
author: " Kim Nguyen"
date: "4/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## I. Introduction

Social media has been evolved and affected our lives in many aspects. In this assignment, we aim to have a closer look at the two most popular social media platforms: Tiktok and Facebook. These platforms are in a similar industry but with very different target audiences, thus the two brands and their audiences could differ in their communication styles and language. Understanding the language used in these platforms may lead to their business implications and directions. 

Our research question focuses on whether there are differences in sentiments of tweet communications between Tiktok and Facebook account. 

## II. Methodology

Our method of sentiments analysis is text mining with R. 

First, after getting tweets from twitter, we use basic tools of data exploration to transform, visualize, and examine different features of the datasets, such as source, time, length, and content (e.g, link and picture) of the tweets. We produce bar charts to visualize the most popular words used by each twitter account, as well as the most popular sentiments associated with tweets that each account produces. A wordcloud also helps paint a clearer picture of each company's most commonly used words.

Second, we transform the datasets into tidy text format for sentiment analysis. The two main lexicons that we use are nrc and affin.  

Finally, we run 4 different models to predict if a tweet was posted by either Facebook or Tiktok. The inputs of these models are the length of the tweet, as well as sentiment (which includes anger, anticipation, disgust, negative, postive, trust, joy, surprise, fear and sadness).  

The first model is a Simple Decision Tree, the second model is a Bagging Model, the third model is a Random Forest and the fourth model is a Gradient Boosting Model.

Our results include a sum of squares analysis on the test set of data to determine which models have the smallest differences between the predicted tweeter and actual tweeter. We also include confusion matrices on the test set of data to analyze the prediction accuracy of the 4 models.


```{r, warning = FALSE, message = FALSE}
#Loading packages.
library(rtweet)
library(tidyverse)
library(lubridate)
library(scales)
library(tidytext)
library(wordcloud)
library(textdata)
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(rpart.plot)
library(ipred)       # for fitting bagged decision trees
library(ranger)
library(gbm)
library(vip)
```


```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE}
api_key <- "0000"
api_secret_key <- "0000"
access_token <- "0000"
access_token_secret <- "0000"
token <- create_token(
  app = "Hw9-alligators",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```



```{r, message = FALSE, warning = FALSE}
#Getting tweets
# Run these two lines to get the tweets 
# and then save them as a csv for future use
# tiktok <- get_timeline("tiktok_us", n=3200)
# tiktok %>% write_as_csv('tiktok.csv')
# 
# facebook <- get_timeline("Facebook", n=3200)
# facebook %>% write_as_csv("facebook.csv")
tiktok <-
  read_csv('tiktok.csv') %>% 
  select(status_id, source, text, created_at)
facebook <-
  read_csv('facebook.csv') %>% 
  select(status_id, source, text, created_at)
nrc <- read_rds("nrc.rds")
facebook %>% head()
```

Each dataset has around 3200 tweets.



```{r}
facebook %>%
  count(source, hour = hour(with_tz(created_at, "EST"))) %>%
  mutate(percent = n/sum(n)) %>%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = "Hour of day (EST)", y = "% of tweets", color = "") + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle('Facebook Source Breakdown by Hour')
tiktok %>%
  count(source, hour = hour(with_tz(created_at, "EST"))) %>%
  mutate(percent = n/sum(n)) %>%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = "Hour of day (EST)", y = "% of tweets", color = "") + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle('Tiktok Source Breakdown by Hour')
```
These above figures indicate Tiktok/Facebook breakdown by hour. Across sources, the "busiest" time on both platforms are from 12:00 to 20:00. While Khoros Publishing has the most tweets about Facebook with its peak around 16:00, Twitter Web App and Fan Experiences (peaks around 16:00) are the main source of tweets about Tiktok. 


```{r}
fb_wordcounts <- 
  facebook %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)
tiktok_wordcounts <- 
  tiktok %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)
writeLines(c(paste0("Facebook Mean Tweet Length: ", 
                  mean(fb_wordcounts$tweetLength)), 
           paste0("TikTok Mean Tweet Length: ", 
                  mean(tiktok_wordcounts$tweetLength))))
hist(tiktok_wordcounts$tweetLength)
hist(fb_wordcounts$tweetLength)
```

In terms of tweet length, a typical tweet related to Tiktok has from 50 to 100 words. There are less tweets that has more than 100 words. A typical tweet related to Facebook has around 150 words. 


```{r}
fb_picture_counts <- 
  facebook %>%
  filter(!str_detect(text, '^"')) %>%
  count(picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
tiktok_picture_counts <- 
  tiktok %>%
  filter(!str_detect(text, '^"')) %>%
  count(picture = ifelse(str_detect(text, "t.co"),
                         "Picture/link", "No picture/link"))
barplot(fb_picture_counts$n, 
        names.arg=c("No picture/link", "Picture/link"),
        main = "Facebook # of Tweets with and without pics/link")
barplot(tiktok_picture_counts$n, 
        names.arg=c("No picture/link", "Picture/link"),
        main = "Tiktok # of Tweets with and without pics/link")
```

Facebook tweets that contains pictures or links are more common than ones that have no pictures or links. There are no remarakble differences between tweets that contain picture/link and ones that don't contains picture/link from Tiktok. 



```{r}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
# Unnest the text strings into a data frame of words
fb_words <- 
  facebook %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
tiktok_words <- 
  tiktok %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
# Inspect the first six rows of tweet_words
head(fb_words)
```

```{r}
fb_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
tiktok_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip()
```

```{r}
fb_sentiment <-    
 inner_join(fb_words, nrc, by = "word") %>% 
            group_by(sentiment)  
tiktok_sentiment <-    
 inner_join(tiktok_words, nrc, by = "word") %>% 
            group_by(sentiment) 
fb_words %>% head()
```

Here we compare the sentiment between Facebook and TikTok. It looks like discussions surrounding Facebook uses more trust words while topics about TikTok uses more words that reflect anticipation.

```{r}
fb_sentiment_analysis <- fb_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
fb_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="Facebook Sentiment")
tiktok_sentiment_analysis <- tiktok_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
tiktok_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="TikTok Sentiment")
```
```{r}
fb_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> fb_sentiment_analysis2
ggplot(fb_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="Facebook Sentiment")
tiktok_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> tiktok_sentiment_analysis2
ggplot(tiktok_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="Tik Tok Sentiment")
```

We also want to visualize common words on Facebook and Tiktok by Wordcloud. The visual depiction indicates to us that "learn", "center" and "report" are common words, with more secondary common words such as "secure" page", and "visit" for Facebook account engagement. This could be that Facebook users tweet about account issues. Whereas, TikTok has "top", "tomorrow", and "prizes" as common words, and more secondary common words such as "winner, "nominating", and "grand", indicating that the social media platform likes to promote competitions or giveaways, which makes sense given their younger demographics might enjoy these types of rewards and games.


```{r, warning = FALSE, message = FALSE}
facebook_cloud <- fb_words  %>% count(word) %>% arrange(-n)
wordcloud(facebook_cloud$word, facebook_cloud$n, max.words = 200, colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"))
tiktok_cloud <- tiktok_words  %>% count(word) %>% arrange(-n)
wordcloud(tiktok_cloud$word, tiktok_cloud$n, max.words = 200, colors = c("#00B2FF", "red", "#FF0099", "#6600CC", "green", "orange", "blue", "brown"))
```


Next, we examine texts on Facebook and Tiktok to see their positive-negative score by using afinn lexicon 

```{r, message = FALSE, warning = FALSE}
# run this to get afinn lexicon and save it as a csv
# get_sentiments ("afinn") -> afinn
#
#afinn %>% write_as_csv("afinn.csv")
afinn <- read_csv('afinn.csv')
```

```{r}
fb_afinn <-    
 inner_join(fb_words, afinn, by = "word") 
tiktok_afinn <-    
 inner_join(tiktok_words, afinn, by = "word")
fb_afinn %>% summarise(mean_fb_afinn = mean(value))
tiktok_afinn %>% summarise(mean_tt_afinn = mean(value))
```

Mean of Facebook's afinn value is 0.79 while mean of Tiktok's afinn value is 1.704293. In general, tweets from Tiktok are more positive than those on Facebook. 


Here, we predict the user based on the tweet length and number of words for each sentiment. TikTok is encoded as 1, and Facebook is encoded as 0.

```{r}
fb_sentiment_counts <- 
  fb_sentiment %>% 
  group_by(status_id) %>% 
  count(sentiment) %>% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)
tiktok_sentiment_counts <- 
  tiktok_sentiment %>% 
  group_by(status_id) %>% 
  count(sentiment) %>% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)
tiktok_feature_selection <- 
  tiktok_wordcounts %>% 
  mutate(user = 1) %>% 
  left_join(tiktok_sentiment_counts, 
            by="status_id")
facebook_feature_selection <-
  fb_wordcounts %>% 
  mutate(user = 0) %>% 
  left_join(fb_sentiment_counts, 
            by="status_id")
both_users <- 
  tiktok_feature_selection %>% 
  rbind(facebook_feature_selection) %>%
  mutate_if(is.numeric,coalesce,0)
set.seed(123)
index <- 
  createDataPartition(both_users$user,
                      p = 0.8, list = FALSE)
for_decisiontree <-
  both_users %>% select(-1,-2,-3,-4)
train <- for_decisiontree[index, ]
test  <- for_decisiontree[-index, ]
simple_model <- rpart(user ~ ., data = train, method = "class")
rpart.plot(simple_model, yesno = 2)
```
Interpretation of decision tree: If a tweet has less than 94 words, it is projected to be about/from Tiktok, with 82% probability, and 29% of the data correspond with this case. In another case, if a tweet has more than/or equal 94 words, 37% chance that it is related to Facebook, with 71% data correspondence. However, within this case, if its anticipation "score" is more than 2, 82% this tweet is related to Tiktok. 



```{r, warning = FALSE}
set.seed(123)
bagging_model <- train(
  user ~ .,
  data = train,
  method = "treebag",
  trControl = trainControl(method = "oob"),
  keepX = T,
  nbagg = 100,
  importance = "impurity",
  control = rpart.control(minsplit = 2, cp = 0))
bagging_model
n_features <- length(setdiff(names(train), "user"))
rf_model <- ranger(
  user ~ .,
  data = train,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = "order",
  importance = "permutation",
  seed = 123)
rf_model
set.seed(123)  # for reproducibility
gbm_model <- gbm(
  formula = user ~ .,
  data = train,
  distribution = "gaussian",  # SSE loss function
  n.trees = 1000,
  shrinkage = 0.05,
  interaction.depth = 5,
  n.minobsinnode = 4,
  cv.folds = 10)
gbm_model
```

```{r}
actual_train <- train$user
simple_pred_train <- 
  predict(simple_model, newdata = train) %>% 
  as_tibble() %>% 
  select(2) %>% 
  unlist() %>% 
  as.vector()
rss_simple_train <- sum((actual_train-simple_pred_train)^2)
bagging_pred_train <- 
  predict(bagging_model, newdata = train) %>% 
  as.vector()
rss_bagging_train <- sum((actual_train-bagging_pred_train)^2)
rf_pred_train <- predict(rf_model, data = train, seed = 123, verbose = T)[1] %>% unlist()
rss_rf_train <- sum((actual_train-rf_pred_train)^2)
gb_pred_train <- predict(gbm_model, newdata = train)
rss_gb_train <- sum((actual_train-gb_pred_train)^2)
cat(paste0("Residual Sum of Squares on Training Set\n",
           "\nSimple model: ", rss_simple_train, 
           "\nBagged model: ", rss_bagging_train, 
           "\nRandom forests model: ", rss_rf_train, 
           "\nGradient boost model: ", rss_gb_train))
```


```{r}
actual_test <- test$user
simple_pred_test <- 
  predict(simple_model, newdata = test) %>% 
  as_tibble() %>% 
  select(2) %>% 
  unlist() %>% 
  as.vector()
rss_simple_test <- sum((actual_test-simple_pred_test)^2)
bagging_pred_test <- 
  predict(bagging_model, newdata = test) %>% 
  as.vector()
rss_bagging_test <- sum((actual_test-bagging_pred_test)^2)
rf_pred_test <- predict(rf_model, data = test, seed = 123, verbose = T)[1] %>% unlist() %>% as.vector()
rss_rf_test <- sum((actual_test-rf_pred_test)^2)
gb_pred_test <- predict(gbm_model, newdata = test)
rss_gb_test <- sum((actual_test-gb_pred_test)^2)
cat(paste0("Residual Sum of Squares on Test Set\n",
           "\nSimple model: ", rss_simple_test, 
           "\nBagged model: ", rss_bagging_test, 
           "\nRandom forests model: ", rss_rf_test, 
           "\nGradient boost model: ", rss_gb_test))
```

The random forests model performed the best on the test set even though it was only second best for the training set. However, that may be an indication that the bagging model was overfit, which caused it to perform much worse on the test set than the random forests model.


## Evaluating Performance: Confusion Matrices

Now, I produce confusion matrices for all tree-based methods---first evaluating their performance on the test set. Note again that a Tiktok tweet is encoded as 1, and a Facebook tweet is encoded as 0. The code is shown for the first matrix but not for subsequent ones for the sake of elegance.

**Simple Model - Test Set:**

```{r}
simple_test_confusion <- 
  confusionMatrix(data = factor(round(simple_pred_test)),
                  reference = factor(actual_test), mode = "prec_recall")
simple_test_errors <- 
  simple_test_confusion$table[2] +
  simple_test_confusion$table[3]
simple_test_accuracy <-
  as.numeric(simple_test_confusion$overall[1])
simple_test_confusion
```
**Bagging Model - Test Set:**

```{r, echo = FALSE}
bagging_test_confusion <- 
  confusionMatrix(data = factor(round(bagging_pred_test)), 
                  reference = factor(actual_test), mode = "prec_recall")
bagging_test_errors <- 
  bagging_test_confusion$table[2] +
  bagging_test_confusion$table[3]
bagging_test_accuracy <-
  as.numeric(bagging_test_confusion$overall[1])
bagging_test_confusion
```
**Random Forests - Test Set:**

```{r, echo = FALSE}
rf_test_confusion <- 
  confusionMatrix(data = factor(round(rf_pred_test)), 
                  reference = factor(actual_test), mode = "prec_recall")
rf_test_errors <- 
  rf_test_confusion$table[2] +
  rf_test_confusion$table[3]
rf_test_accuracy <-
  as.numeric(rf_test_confusion$overall[1])
rf_test_confusion
```

**Gradient Boosting Model - Test Set:**

```{r, echo = FALSE}
gb_test_confusion <- 
  confusionMatrix(data = factor(round(gb_pred_test)), 
                  reference = factor(actual_test), mode = "prec_recall")
gb_test_errors <- 
  gb_test_confusion$table[2] +
  gb_test_confusion$table[3]
gb_test_accuracy <-
  as.numeric(gb_test_confusion$overall[1])
gb_test_confusion
```


## III. Results

Our sentiment analyses indicate that there are differences in sentiments of tweet communications between Tiktok and Facebook account. In general, while discussions surrounding Facebook uses more trust words, topics about TikTok uses more words that reflect anticipation. Tweets from Tiktok are more positive than those on Facebook. 


When testing the models on our test data sets, for the simple model we observed 81.38% accuracy, for the bagging model we observed 80.83% accuracy, for the random forest we observed 83.41% accuracy, and for the gradient boosting model we observed 84.43% accuracy. Another good metric to compare is balanced accuracy, which is the average of specificity and sensitivity. For the simple model, balanced accuracy is 81.3%. For the bagging model, balanced accuracy is 80.83%. For the random forest, balanced accuracy is 83.37%. For the gradient boosting model, balanced accuracy is 84.36%.  

Therefore, it seems that in terms of prediction accuracy, the rank of our 4 models is as follows:  
1. Gradient Boosting Model  
2. Random Forest  
3. Simple Model  
4. Bagging Model

When analyzing the sum of squares difference between the actual tweeter and the predicted tweeter, the results were as follows:  

Simple model: 175.391975607092  
Bagged model: 170.061500301201  
Random forests model: 146.952559929777  
Gradient boost model: 151.839521942722  

In all, these results show us that the best model is a toss up between the Random Forest Model and the Gradient Boosting Model.




## IV. Conclusion

Looking at the analyses, it seems that the Facebook and TikTok account and users tweet about these two social media platforms for different reasons. People tweeting about Facebook are likely to be reporting their account issues, which are associated with words such as "secure" and "trust." Whereas, people tweeting about TikTok seem to be participating in prize giveaways, which is associated with "anticipation" words such as "winning" and "tomorrow." Overall, these analyses make sense given the different demographics of the two social media platforms. 

## V. Contributions
Ammar Plumber, Elaina Lin, Kim Nguyen, Meghan Aines, Ryan Karbowicz