---
title: "COVID-19 Tweets in Mar 2020 & 2021"
output: flexdashboard::flex_dashboard
---
<style>
.ourtext {
  margin-left:12em;
  margin-right:12em;
}

.navbar-inverse {
background-color: #011F5B !important;
}

.storyboard-nav .sbframelist ul li.active {
    background-color: #6aade2;
}
</style>

```{r setup, include=FALSE}
options(scipen = 999)
library(flexdashboard)
library(plotly)
library(htmltools)
library(rmarkdown)
library(tidyverse)
library(scales)
library(tidytext)
library(wordcloud)
library(textdata)
library(ggrepel)
library(ranger)
library(caret)
library(vip)
library(kableExtra)
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(pals)
library(quanteda)
library(seededlda)
library(newsmap)
library(maps)
library(LSX)
library(quanteda.textstats)
library(quanteda.textplots)
library(SnowballC)
library(gt)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
twenty <- 
  read_csv('03-30-2020.csv', 
           col_types = cols(id = col_character())) %>% 
  select(id, text = full_text, year)
twentyone <- 
  read_csv('03-30-2021.csv', 
           col_types = cols(id = col_character())) %>% 
  select(id, text = full_text, year)
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
twenty_words <- 
  twenty %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
twentyone_words <- 
  twentyone %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
twenty_most_common <- 
  twenty_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n))
graph1 <-
  ggplotly(
  twenty_most_common %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip() + 
  ggtitle("2020 COVID-19 Tweets Word Frequency")
  )
twentyone_most_common <- 
  twentyone_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n))
graph2 <- 
  ggplotly(
  twentyone_most_common %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip() +
  ggtitle("2021 COVID-19 Tweets Word Frequency")
  )
#added - common words by proportion
compare_most_common <- full_join(twenty_most_common, twentyone_most_common, by = "word")
colnames(compare_most_common) <- c("Word", "2020", "2021")
# most common words by proportion 2020
compare_most_common %>%  mutate(`2020 proportion` = compare_most_common$`2020`/4666) -> prp_twenty_most_common
prp_twenty_most_common[!is.na(prp_twenty_most_common$`2020 proportion`),] -> prp_twenty_most_common
prp_twenty_most_common %>%  select("Word",`2020 proportion`) %>% arrange(-`2020 proportion`) -> prp_twenty_most_common
# most common words by proportion 2021
compare_most_common %>%  mutate(`2021 proportion` = compare_most_common$`2021`/5341) -> prp_twentyone_most_common
prp_twentyone_most_common[!is.na(prp_twentyone_most_common$`2021 proportion`),] -> prp_twentyone_most_common
prp_twentyone_most_common %>%  select("Word",`2021 proportion`) %>% arrange(-`2021 proportion`) -> prp_twentyone_most_common
#graph
prpgraph_twenty_most_common <-
  ggplotly( 
  prp_twenty_most_common %>%
  ggplot(aes(x = Word, y = `2020 proportion`)) +
  geom_bar(stat = "identity" , fill="firebrick") +
  xlab("word") +
  ylab("Relative Frequency") +
  coord_flip() + 
  ggtitle("2020 COVID-19 Tweets Words Rel. Freq.")
  )
prpgraph_twentyone_most_common <-
  ggplotly( 
  prp_twentyone_most_common %>%
  ggplot(aes(x = reorder(Word, `2021 proportion`), y = (`2021 proportion`)) ) +
  geom_bar(stat = "identity", fill="firebrick") +
  xlab("word") +
  ylab("Relative Frequency") +
  coord_flip() + 
  ggtitle("2021 COVID-19 Tweets Words Rel. Freq.")
  )
  
## 
twenty_cloud <- 
  twenty_words  %>% 
  count(word) %>% 
  arrange(-n)
twentyone_cloud <- 
  twentyone_words  %>% 
  count(word) %>% 
  arrange(-n)
twenty_wordcounts <- 
  twenty %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)
twentyone_wordcounts <- 
  twentyone %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)
nrc <- read_rds("nrc.rds")
twenty_sentiment <-
  inner_join(twenty_words, nrc, by = "word")
twentyone_sentiment <-
  inner_join(twentyone_words, nrc, by = "word")
twenty_sentiment_analysis <- 
  twenty_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
graph3 <- 
  ggplotly(
    twenty_sentiment_analysis %>% 
      summarize(sum(n)) %>% 
      ungroup() %>% 
      top_n(15) %>% 
      ggplot(aes(x = sentiment, y = `sum(n)` )) +
      geom_bar(stat = "identity", fill="firebrick") +
      coord_flip() +
      ylab("Frequency") +
      xlab("Sentiment") +
      labs(title="30 Mar 2020 Sentiments")
  )
twentyone_sentiment_analysis <- 
  twentyone_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
graph4 <-
  ggplotly(
    twentyone_sentiment_analysis %>%
      summarize(sum(n)) %>% 
      ungroup() %>% 
      top_n(15) %>% 
      ggplot(aes(x = sentiment, y = `sum(n)` )) +
      geom_bar(stat = "identity", fill="firebrick") +
      coord_flip() +
      ylab("Frequency") +
      xlab("Sentiment") +
      labs(title="30 Mar 2021 Sentiments")
  )
twenty_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twenty_sentiment_analysis2
graph5 <-
  ggplotly(
    ggplot(twenty_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
    facet_wrap(~ sentiment, scales = "free")+ 
    geom_bar(stat ="identity", fill="firebrick") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(y="count", title="30 Mar 2020 Sentiment Words")
  )
twentyone_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twentyone_sentiment_analysis2
graph6 <-
  ggplotly(
  ggplot(twentyone_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity", fill="firebrick") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2021 Sentiment Words")
  )
# sentiment breakdown
twenty_sentiment_analysis2 %>%
  ggplot(aes(x=n, y=1, label=word, fill = sentiment)) +
  facet_grid(~sentiment) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3, show.legend= FALSE
                   ) +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_blank(),
        panel.border = element_rect("lightgray", fill = NA)
        ) +  guides(fill=FALSE) + 
  xlab("Count") + ylab("Words") +
  ggtitle("30 Mar 2020 Sentiment Breakdown") + 
  coord_flip()  -> twenty_sentiment_analysis3
twenty_sentiment_analysis2 %>%
  ggplot(aes(x=n, y=1, label=word)) + 
  facet_grid(~sentiment) + 
  geom_point(color = "transparent") + 
  geom_text(aes(color=sentiment)) +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, face="bold", size=17, 
                                  lineheight = 0.9),
        axis.title = element_text(face="bold",size = (12))) + 
  xlab("Count") + ylab("Words") +
  ggtitle("30 Mar 2021 Sentiment Breakdown") + 
  coord_flip()  -> twenty_sentiment_analysis4
ggplotly(twenty_sentiment_analysis4, 
         tooltip=c("sentiment","word","n")) ->  twenty_sentiment_analysis4
twenty_sentiment_analysis4
twentyone_sentiment_analysis2 %>%
  ggplot(aes(x=n, y=1, label=word, fill = sentiment)) +
  facet_grid(~sentiment) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3, show.legend= FALSE
                   ) +
    theme(panel.grid = element_blank(), 
        axis.text.x = element_blank(),
        panel.border = element_rect("lightgray", fill = NA)
        ) +  guides(fill=FALSE) + 
  xlab("Count") + ylab("Words") +
  ggtitle("30 Mar 2021 Sentiment Breakdown") + 
  coord_flip()  -> twentyone_sentiment_analysis3
twentyone_sentiment_analysis2 %>%
  ggplot(aes(x=n, y=1, label=word)) + 
  facet_grid(~sentiment) + 
  geom_point(color = "transparent") + 
  geom_text(aes(color=sentiment)) +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, face="bold", size=17, 
                                  lineheight = 0.9),
        axis.title = element_text(face="bold",size = (12))) + 
  xlab("Count") + ylab("Words") +
  ggtitle("30 Mar 2021 Sentiment Breakdown") + 
  coord_flip() -> twentyone_sentiment_analysis4
ggplotly(twentyone_sentiment_analysis4, 
         tooltip=c("sentiment","word","n")) ->  twentyone_sentiment_analysis4
twentyone_sentiment_analysis4
afinn <- read_csv('afinn.csv')
twenty_afinn <-    
 inner_join(twenty_words, 
            afinn, 
            by = "word")
twenty_afinn %>% select(word, value) %>% distinct() -> twenty_afinn2
twenty_afinn %>% count(word) -> twenty_afinn_count
right_join(twenty_afinn_count, 
            twenty_afinn2, 
            by= "word") -> twenty_afinn_count
colnames(twenty_afinn_count) <- c("Word", "Count", "Value")
twenty_afinn_count[order(twenty_afinn_count$Count, decreasing = TRUE),]  -> twenty_afinn_count
       
twenty_afinn_count %>%  mutate(highlgt = ifelse(Value >= 1, "#65d69a", "#ff546b")) -> twenty_afinn_count
twenty_afinn_title <- list(
  size = 15)
twenty_afinn_table <- plot_ly(
  type = 'table',
  columnorder = c(0, 1,2),
  header = list(
    values = c("Word", "Count", "Value"),
    align = c("center", "center"),
    line = list(width = 1, color = 'black'),
  #  fill = list(color = c("#1DA1F2","#1DA1F2")),
    font = list(size = 14)
  ),
  cells = list(
    values = rbind(as.character(twenty_afinn_count$Word), as.character(twenty_afinn_count$Count), twenty_afinn_count$Value),
    align = c("center", "center"),
    line = list(color = "black", width = 1),
    font = list(size = 13, color = c("black")),
    fill = list(color = list(twenty_afinn_count$highlgt))
  ))  %>%  layout(title = "2020 AFINN Scores By Word", font = twenty_afinn_title, margin = list(t = -8))
twentyone_afinn <-    
 inner_join(twentyone_words, 
            afinn, 
            by = "word")
twentyone_afinn %>% select(word, value) %>% distinct() -> twentyone_afinn2
twentyone_afinn %>% count(word) -> twentyone_afinn_count
right_join(twentyone_afinn_count, 
            twentyone_afinn2, 
            by= "word") -> twentyone_afinn_count
colnames(twentyone_afinn_count) <- c("Word", "Count", "Value")
twentyone_afinn_count[order(twentyone_afinn_count$Count, decreasing = TRUE),]  -> twentyone_afinn_count
       
twentyone_afinn_count %>%  mutate(highlgt = ifelse(Value >= 1, "#65d69a", "#ff546b")) -> twentyone_afinn_count
twentyone_afinn_title <- list(
  size = 15)
twentyone_afinn_table <- plot_ly(
  type = 'table',
  columnorder = c(0, 1,2),
  header = list(
    values = c("Word", "Count", "Value"),
    align = c("center", "center"),
    line = list(width = 1, color = 'black'),
  #  fill = list(color = c("#1DA1F2","#1DA1F2")),
    font = list(size = 14)
  ),
  cells = list(
    values = rbind(as.character(twentyone_afinn_count$Word), as.character(twentyone_afinn_count$Count), twentyone_afinn_count$Value),
    align = c("center", "center"),
    line = list(color = "black", width = 1),
    font = list(size = 13, color = c("black")),
    fill = list(color = list(twentyone_afinn_count$highlgt))
  ))  %>%  layout(title = "2021 AFINN Scores By Word", font = twentyone_afinn_title, margin = list(t = -8) )
twenty_mean_afinn <- 
  twenty_afinn %>% 
  summarise(mean = mean(value))
twentyone_mean_afinn <- 
  twentyone_afinn %>% 
  summarise(mean_tt_afinn = mean(value))
twenty_nested <- 
  twenty_words %>% 
  group_by(id) %>% 
  nest(word = word) %>% 
  mutate(text = paste(word %>% unlist(), collapse = ' ')) %>% 
  ungroup()
twenty_toDfm <- 
  lapply(X = twenty_nested$text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()
twenty_dfm <- 
  dfm(twenty_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)
set.seed(1)
tmod_twenty <- textmodel_lda(twenty_dfm, k = 10)
# assign topic as a new document-level variable
twenty_dfm$topic <- topics(tmod_twenty)
twentyone_nested <- 
  twentyone_words %>% 
  group_by(id) %>% 
  nest(word = word) %>% 
  mutate(text = paste(word %>% unlist(), collapse = ' ')) %>% 
  ungroup()
twentyone_toDfm <- 
  lapply(X = twentyone_nested$text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()
twentyone_dfm <- 
  dfm(twentyone_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)
set.seed(1)
tmod_twentyone <- textmodel_lda(twentyone_dfm, k = 10)
# assign topic as a new document-level variable
twenty_dfm$topic <- topics(tmod_twenty)
twentyone_dfm$topic <- topics(tmod_twentyone)
toks_tweets <- tokens(twenty_toDfm, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")
dfmat_tweets <- dfm(toks_tweets)
tstat_freq_twenty <- textstat_frequency(dfmat_tweets, n = 30)
toks_tweets <- tokens(twentyone_toDfm, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")
dfmat_tweets <- dfm(toks_tweets)
tstat_freq_twentyone <- textstat_frequency(dfmat_tweets, n = 30)
toks_news20 <- 
  tokens(twenty_toDfm, remove_punct = TRUE) %>% 
  tokens_remove(pattern = c(stopwords("en")), 
                valuetype = "fixed", padding = TRUE)
toks_label20 <- 
  tokens_lookup(toks_news20, 
                dictionary = data_dictionary_newsmap_en, 
                levels = 3) # level 3 is countries
dfmat_label20 <- 
  dfm(toks_label20, tolower = FALSE)
dfmat_feat20 <- 
  dfm(toks_news20, tolower = TRUE)
dfmat_feat_select20 <- 
  dfm_select(dfmat_feat20, 
             pattern = "^[A-Z][A-Za-z0-9]+", 
             valuetype = "regex", 
             case_insensitive = TRUE) %>% 
  dfm_trim(min_termfreq = 10)
toks_news21 <- 
  tokens(twentyone_toDfm, remove_punct = TRUE) %>% 
  tokens_remove(pattern = c(stopwords("en")), 
                valuetype = "fixed", padding = TRUE)
toks_label21 <- 
  tokens_lookup(toks_news21, 
                dictionary = data_dictionary_newsmap_en,
                levels = 3) # level 3 is countries
dfmat_label21 <- 
  dfm(toks_label21, tolower = FALSE)
dfmat_feat21 <- 
  dfm(toks_news21, tolower = TRUE)
dfmat_feat_select21 <- 
  dfm_select(dfmat_feat21, 
             pattern = "^[A-Z][A-Za-z0-9]+", 
             valuetype = "regex", 
             case_insensitive = TRUE) %>% 
  dfm_trim(min_termfreq = 10)
newsmap20 <- textmodel_newsmap(dfmat_feat_select20, y = dfmat_label20)
pred_nm20 <- predict(newsmap20)
count20 <- sort(table(factor(pred_nm20, 
                             levels = colnames(dfmat_label20))), 
                decreasing = TRUE)
newsmap21 <- textmodel_newsmap(dfmat_feat_select21, y = dfmat_label21)
pred_nm21 <- predict(newsmap21)
count21 <- sort(table(factor(pred_nm21, 
                             levels = colnames(dfmat_label21))), 
                decreasing = TRUE)
dat_country20 <- as.data.frame(count20, stringsAsFactors = FALSE)
colnames(dat_country20) <- c("id", "frequency")
dat_country21 <- as.data.frame(count21, stringsAsFactors = FALSE)
colnames(dat_country21) <- c("id", "frequency")
world_map <- map_data(map = "world")
world_map$region <- iso.alpha(world_map$region) # convert country name to ISO code
graph7 <-
  ggplotly(
  ggplot(dat_country20, aes(map_id = id)) +
  geom_map(aes(fill = frequency), map = world_map) +
  expand_limits(x = world_map$long, y = world_map$lat) +
  scale_fill_continuous(name = "Frequency") +
  theme_void() +
  coord_fixed() + 
  ggtitle("30 Mar 2020: Number of Country Mentions on Twitter")
  )
graph8 <-
  ggplotly(
  ggplot(dat_country21, aes(map_id = id)) +
  geom_map(aes(fill = frequency), map = world_map) +
  expand_limits(x = world_map$long, y = world_map$lat) +
  scale_fill_continuous(name = "Frequency") +
  theme_void() +
  coord_fixed() + 
  ggtitle("30 Mar 2021: Number of Country Mentions on Twitter")
  )
twenty_rts <-
  read_csv('2020_retweets.csv') %>% 
  select(-1)
twentyone_rts <-
  read_csv('2021_retweets.csv') %>% 
  select(-1)
twenty_uqtweets <- 
  twenty_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))
twenty_rts_toDfm <- 
  lapply(X = twenty_rts$Text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()
twenty_rts_dfm <- 
  dfm(twenty_rts_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)
twenty_rts_dfm$topic <- 
  predict(
    tmod_twenty,
    newdata = twenty_rts_dfm,
    max_iter = 2000,
    verbose = quanteda_options("verbose")
    )
twentyone_rts_toDfm <- 
  lapply(X = twentyone_rts$Text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()
twentyone_rts_dfm <- 
  dfm(twentyone_rts_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)
twentyone_rts_dfm$topic <- 
  predict(
    tmod_twentyone,
    newdata = twentyone_rts_dfm,
    max_iter = 2000,
    verbose = quanteda_options("verbose")
    )
twenty_terms <- terms(tmod_twenty, 20) %>% as.data.frame()
twenty_termslist <- c()
for (x in 1:10) {
  twenty_termslist <- 
    append(twenty_termslist, twenty_terms %>% select(x) %>% 
             unname() %>% unlist())
}
twenty_termslist <- twenty_termslist %>% unique()
#Getting topic-related terms for 2021
twentyone_terms <- terms(tmod_twentyone, 20) %>% as.data.frame()
twentyone_termslist <- c()
for (x in 1:10) {
  twentyone_termslist <- 
    append(twentyone_termslist, twentyone_terms %>% select(x) %>% 
             unname() %>% unlist())
}
twentyone_termslist <- twentyone_termslist %>% unique()
twenty_rts_words <- 
  twenty_rts %>%
  filter(!str_detect(Text, '^"')) %>%
  mutate(text = str_replace_all(Text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>% 
  mutate(word = wordStem(word))
twentyone_rts_words <- 
  twentyone_rts %>%
  filter(!str_detect(Text, '^"')) %>%
  mutate(text = str_replace_all(Text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>% 
  mutate(word = wordStem(word))
twenty_rts_word_predict <- 
  twenty_rts_words %>% 
  filter(word %in% twenty_termslist) %>% 
  group_by(Text) %>% 
  count(word) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = word, 
              values_from = n,
              values_fill = 0)
twentyone_rts_word_predict <- 
  twentyone_rts_words %>% 
  filter(word %in% twentyone_termslist) %>% 
  group_by(Text) %>% 
  count(word) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = word, 
              values_from = n,
              values_fill = 0)
twenty_rts_wordcounts <- 
  twenty_rts %>%
  mutate(tweetLength = str_length(Text)) %>% 
  filter(tweetLength < 500)
twentyone_rts_wordcounts <- 
  twentyone_rts %>%
  mutate(tweetLength = str_length(Text)) %>% 
  filter(tweetLength < 500)
twenty_rts_sentiment <-
  inner_join(twenty_rts_words, nrc, by = "word")
twentyone_rts_sentiment <-
  inner_join(twentyone_rts_words, nrc, by = "word")
twenty_rts_sentiment_analysis <- 
  twenty_rts_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
graph9 <-
  ggplotly(
  twenty_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2020 Sentiments")
  )
twentyone_rts_sentiment_analysis <- 
  twentyone_rts_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)
graph10 <-
  ggplotly(
  twentyone_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2021 Sentiments")
  )
twentyone_rts_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twenty_rts_sentiment_analysis2
graph11 <-
  ggplotly(
  ggplot(twenty_rts_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2020 Sentiment Words")
  )
twentyone_rts_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twentyone_rts_sentiment_analysis2
graph12 <-
  ggplotly(
  ggplot(twentyone_rts_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2021 Sentiment Words")
  )
twenty_rts_afinn <-    
 inner_join(twenty_rts_words, 
            afinn, 
            by = "word")
twenty_rts_afinn %>% select(word, value) %>% distinct() -> twenty_rts_afinn2
twenty_rts_afinn %>% count(word) -> twenty_rts_afinn_count
right_join(twenty_rts_afinn_count, 
            twenty_rts_afinn2, 
            by= "word") -> twenty_rts_afinn_count
colnames(twenty_rts_afinn_count) <- c("Word", "Count", "Value")
twenty_rts_afinn_count[order(twenty_rts_afinn_count$Count, decreasing = TRUE),]  -> twenty_rts_afinn_count
       
twenty_rts_afinn_count %>%  mutate(highlgt = ifelse(Value >= 1, "#65d69a", "#ff546b")) -> twenty_rts_afinn_count
twenty_rts_afinn_title <- list(
  size = 15)
twenty_rts_afinn_table <- plot_ly(
  type = 'table',
  columnorder = c(0, 1,2),
  header = list(
    values = c("Word", "Count", "Value"),
    align = c("center", "center"),
    line = list(width = 1, color = 'black'),
  #  fill = list(color = c("#1DA1F2","#1DA1F2")),
    font = list(size = 14)
  ),
  cells = list(
    values = rbind(as.character(twenty_rts_afinn_count$Word), as.character(twenty_rts_afinn_count$Count), twenty_rts_afinn_count$Value),
    align = c("center", "center"),
    line = list(color = "black", width = 1),
    font = list(size = 13, color = c("black")),
    fill = list(color = list(twenty_rts_afinn_count$highlgt))
  ))  %>%  layout(title = "2020 AFINN Scores By Word", font = twenty_rts_afinn_title, margin = list(t = -8))
twentyone_rts_afinn <-    
 inner_join(twentyone_rts_words, 
            afinn, 
            by = "word")
twentyone_rts_afinn %>% select(word, value) %>% distinct() -> twentyone_rts_afinn2
twentyone_rts_afinn %>% count(word) -> twentyone_rts_afinn_count
right_join(twentyone_rts_afinn_count, 
            twentyone_rts_afinn2, 
            by= "word") -> twentyone_rts_afinn_count
colnames(twentyone_rts_afinn_count) <- c("Word", "Count", "Value")
twentyone_rts_afinn_count[order(twentyone_rts_afinn_count$Count, decreasing = TRUE),]  -> twentyone_rts_afinn_count
       
twentyone_rts_afinn_count %>%  mutate(highlgt = ifelse(Value >= 1, "#65d69a", "#ff546b")) -> twentyone_rts_afinn_count
twentyone_rts_afinn_title <- list(
  size = 15)
twentyone_rts_afinn_table <- plot_ly(
  type = 'table',
  columnorder = c(0, 1,2),
  header = list(
    values = c("Word", "Count", "Value"),
    align = c("center", "center"),
    line = list(width = 1, color = 'black'),
  #  fill = list(color = c("#1DA1F2","#1DA1F2")),
    font = list(size = 14)
  ),
  cells = list(
    values = rbind(as.character(twentyone_rts_afinn_count$Word), as.character(twentyone_rts_afinn_count$Count), twentyone_rts_afinn_count$Value),
    align = c("center", "center"),
    line = list(color = "black", width = 1),
    font = list(size = 13, color = c("black")),
    fill = list(color = list(twentyone_rts_afinn_count$highlgt))
  ))  %>%  layout(title = "2021 AFINN Scores By Word", font = twentyone_rts_afinn_title, margin = list(t = -8) )
twenty_rts_mean_afinn <- 
  twenty_rts_afinn %>% 
  summarise(mean = mean(value))
twentyone_rts_mean_afinn <- 
  twentyone_afinn %>% 
  summarise(mean = mean(value))
twenty_rtweet_afinn <- 
  twenty_rts_afinn %>% 
  group_by(Text) %>% 
  summarize(afinn = mean(value))
twenty_rts_sentiment_counts <- 
  twenty_rts_sentiment %>% 
  group_by(Text) %>% 
  count(sentiment) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)
twenty_rts_feature_selection <- 
  twenty_rts_wordcounts %>% 
  left_join(twenty_rts_sentiment_counts, 
            by="Text") %>% 
  left_join(twenty_rtweet_afinn,
            by="Text") %>% 
  left_join(twenty_rts_word_predict,
            by = "Text") %>% 
  mutate(id = row_number()) %>% 
  filter(Followers > 0) %>% 
  mutate(virality = Retweets/Followers)
twentyone_rtweet_afinn <- 
  twentyone_rts_afinn %>% 
  group_by(Text) %>% 
  summarize(afinn = mean(value))
twentyone_rts_sentiment_counts <- 
  twentyone_rts_sentiment %>% 
  group_by(Text) %>% 
  count(sentiment) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)
twentyone_rts_feature_selection <- 
  twentyone_rts_wordcounts %>% 
  left_join(twentyone_rts_sentiment_counts, 
            by="Text") %>% 
  left_join(twentyone_rtweet_afinn,
            by="Text") %>% 
  left_join(twentyone_rts_word_predict,
            by = "Text") %>% 
  mutate(id = row_number()) %>% 
  filter(Followers > 0) %>% 
  mutate(virality = Retweets/Followers)
for_decisiontree_twenty <-
  twenty_rts_feature_selection %>% select(-1,-2,-3,-4,-5,-6,-`id`) %>% drop_na()
colnames(for_decisiontree_twenty) <- make.names(colnames(for_decisiontree_twenty))
n_features <- length(setdiff(names(for_decisiontree_twenty), "virality"))
rf_model20 <- ranger(
  virality ~ .,
  data = for_decisiontree_twenty,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = "order",
  importance = "permutation",
  seed = 123
)
for_decisiontree_twentyone <-
  twentyone_rts_feature_selection %>% select(-1,-2,-3,-4,-5,-6,-`id`) %>% drop_na()
colnames(for_decisiontree_twentyone) <- make.names(colnames(for_decisiontree_twentyone))
n_features <- length(setdiff(names(for_decisiontree_twentyone), "virality"))
rf_model21 <- ranger(
  virality ~ .,
  data = for_decisiontree_twentyone,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = "order",
  importance = "permutation",
  seed = 123
)
```


I. Introduction
=========================================

<div class="ourtext">
```{r, echo = FALSE}
h2("Authors and Acknowledgments")
```


**For Dashboard view (interactive features), please access here: https://516alligators.github.io/FinalProj.html**

Authors: Ammar Plumber, Elaina Lin, Kim Nguyen, Ryan Karbowicz, and Meghan Aines

This website was produced as a final project for _BDS 516: Data Science and Quantitative Modeling_, a graduate course taught by Alex Shpenev at the University of Pennsylvania.

The tweets that we use in this analysis were obtained from the following GitHub repository by Emily Chen, a computer science Ph.D. student at USC: https://github.com/echen102/COVID-19-TweetIDs


```{r, echo = FALSE}
h2("A. Motivation")
```


COVID-19 has wreaked unprecedented havoc around the world. From a data analytics perspective, never before has a pandemic occurred during a time in history when almost any human can publicly share their thoughts on a global platform. More specifically, Twitter offers real-time insight on the attitudes, beliefs, and general moods of a populace. In this analysis, we compare the sentiments of COVID-19 related tweets at the beginning of the pandemic on March 30, 2020, to the sentiments exactly one year later on March 30, 2021. We select this time frame to capture two of the key events during this pandemic, the onset of stay-at-home orders and vaccine availability. As opposed to other data collection methods, such as interviews and surveys (and the numerous response biases that come along with them), sentiment analysis through Twitter is better able to capture the raw and unfiltered emotions of people who feel the need to express their views.

By gaining a better understanding of the general sentiment of a given population, policy leaders can become better informed regarding how to more effectively govern people during times of crisis. For instance, if feelings of fear are high, politicians can offer words of reassurance to instill feelings of calmness and ease. Or, if feelings of trust are low, politicians can attempt to mend the public trust by strengthening accountability and transparency within the government. At the end of the day, essentially any policy decision can be better informed by knowing how the general populace feels about the issue at hand. 

We also examine which COVID-19 topics are most talked about.  Similar to sentiment analysis, topic analysis can help inform policy leaders about which topics garner the most interest and need to be addressed. For instance, in the case of COVID-19, if a popular topic is the lack of ventilators, a good policy leader would be wise to offer updates on the distribution, as well as the known efficacy of ventilators.

We also examine differences in geographic attention between the two dates.  More specifically, we look at how often each country is mentioned in each sample period, as well as which words are associated with each country. In the case of COVID-19, this information can be very helpful in terms of gaining a better understanding of general attitudes towards China. Because the corona virus originated in Wuhan, China, people around the world have unfortunately expressed negative sentiment towards Chinese people. Gaining a better understanding of how these attitudes have changed over time can help inform policy leaders as to whether or not extra measures need to be taken to protect and defend people of Chinese origin.

Lastly, we examine which features are most predictive of how many retweets a tweet gets. In general, it is believed that social media posts with more extreme positive/negative valence tend to be more likely to go viral. In fact, the best selling author Seth Godin has remarked that ”One of the problems with social media is that the stronger the view you express, the more likely it will become amplified.” By examining the kinds of sentiments associated with more viral tweets, as well as the sources of these tweets and the textual elements of the tweets, we can either confirm or disconfirm this common belief. The results that we find can help inform policy leaders about how to craft tweets containing important information in a way that maximizes the likelihood that a large number of people will be exposed to that content. This analysis will also provide valuable insights regarding how the virality of tweets has changed over time, so that policy leaders can adjust their approaches to the climate of the times.


```{r, echo = FALSE}
h2("B. Research Questions")
```


1. Are there differences in sentiments between the two sample periods—both in original tweets and in retweets?

2. Which topics are there the most original tweets about, and which are more often the subject of retweets?

3. Is there a difference in geographic attention between the two dates? For example, was China being discussed more in 2020 or 2021?

4. Which of these features are most predictive of how many retweets a tweet gets? 

5. Are there certain sentiments or topic-specific words that are most likely to attract retweets?

</div>


II. Methods
=========================================

<div class="ourtext">

```{r, echo = FALSE}
h2("Methods")
```


To examine the differences in sentiment between March 30, 2020 and March 31, 2021, we use sentiment analysis through text mining in R and use several methods to visualize and interpret the different aspects of the tweet, including length, content, and source. We use random forests method and backward stepwise regression to predict the virality of the tweets based on selected variables. 

We chose to use **random forests** as

1. it works well with both categorical and continuous values

2. it reduces overfitting in decision trees, leading to a higher accuracy

**Backward stepwise regression** is chosen as 

1. it is faster than other models 

2. it shows us the removed/added variables, providing valuable information about the quality of the predictor variables.

First, we used tidytext and transformed the dataset of COVID-19 related tweets and retweets for sentiment analysis. We manipulate the text with dplyr for further text analysis to determine the frequency of words in the tweets, tweet lengths, common hashtags, and mentions. To conduct further analyses, we use statistical methods to determine the significance in different mean tweet lengths.

Then, we use sentiment analysis to further examine the difference between the two periods in sentiments and word choice. We joined the NRC Word-Emotion Association Lexicon to our data. Doing so allowed us to classify the words with basic emotions and sentiments, providing us an overall picture of the tweet sentiments. 

To rate the valence of the words, we joined the AFINN sentiment lexicon, which attributed a score based on emotional valence. The scores provided a more objective comparison of tweets and retweets between the two periods.

Next, we use the quanteda package for topic modeling to examine prevalent topics and discussions. This algorithm determines the clusters of words that are likely to be associated, thus defining the topics. Topic modeling provides us insight as to the different conversations associated with COVID-19 people are having during this period.

We use the packcircles package to see which account was the most followed and getting retweeted on those days. Packcircles allowed us to visualize the comparisons between the different accounts.

To predict the virality of a tweet, we use the sentiments, mean AFINN score, and topic-specific words as our variables. We use a tweet’s retweet-to-follower ratio to assess the virality of a tweet. We create two models for the two periods - random forests model and backward stepwise regression optimized using Akaike Information Criterion (AIC). We used random forest because it is an ensemble algorithm that runs well on large datasets and has a lower risk of overfitting. Furthermore, this method ensures overall variance and error is minimized. We use backward stepwise regression so we can start with a full model with our selected variables. Using AIC determines the variables to be included by setting the threshold based on the degrees of freedom for the selected variables.

</div>

III. Original Tweets {.storyboard}
=========================================

### What words appear most frequently in tweets from each sample period? 

```{r}
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  div(prpgraph_twenty_most_common, style = "width: 50%;"),
  tags$br(),
  div(prpgraph_twentyone_most_common, style = "width: 50%;")
))
```

***

It appears that by 2021, the term "coronavirus" has become a fairly uncommon way to refer to the virus, with COVID becoming the reference of choice. It also seems that over the time frame, messaging about staying home, social distancing, and Trump has died down and become more undifferentiated from the many other COVID-19-related topics being discussed.

### Is there a difference in tweet lengths between the two samples?

```{r}
print1st <-
  paste0("30 Mar 2020 Mean Tweet Length: ", round(mean(twenty_wordcounts$tweetLength),2))
print2nd <-
  paste0("30 Mar 2021 Mean Tweet Length: ", round(mean(twentyone_wordcounts$tweetLength),2))
hist1 <- 
  ggplotly(
  ggplot(twenty_wordcounts, aes(x=tweetLength)) + geom_histogram(fill="firebrick") + labs(x = "Character Count") + ggtitle("30 Mar 2020 - Histogram of Tweet Lengths")
  )
hist2 <-
  ggplotly(
  ggplot(twentyone_wordcounts, aes(x=tweetLength)) + geom_histogram(fill="firebrick") + labs(x = "Character Count") + ggtitle("30 Mar 2021 - Histogram of Tweet Lengths")
  )
test <- t.test(x = twenty_wordcounts$tweetLength, 
               y = twentyone_wordcounts$tweetLength, 
               alternative = "less", var.equal = FALSE)
ttest <- paste(capture.output(test), collapse = "<br>")
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  p(print1st, tags$br(), print2nd, style = "width: 100%; border: dashed; color: black; font-family: sans-serif;"),
  tags$br(),
  div(hist1, style = "width: 50%;"),
  div(hist2, style = "width: 50%;")
))
```

***

We check whether there is a difference in tweet lengths between the two samples.

Tweets from March 2020 were on average shorter than those from 2021. It seems that there is a greater share of longer tweets (around 300 characters) in 2021.

The difference in mean tweet length is statistically significant, as p-value obtained for the one-sided unpaired two sample t-test is much less than 0.01. The difference is substantial as well, about sixteen fewer characters.

### What differences do we observe in sentiments between the two periods?

```{r}
sent1 <-
  HTML(ggplot_image(twenty_sentiment_analysis3, height = px(500))) 
sent2 <-
  HTML(ggplot_image(twentyone_sentiment_analysis3, height = px(500))) 
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  div(graph3, style = "width: 50%;"),
  div(graph4, style = "width: 50%;"),
  p(" ", style = "width: 100%;"),
  tags$br(),
  div(sent1, style = "width: 50%;"),
  div(sent2, style = "width: 50%;"),
  p(" ")
))
```


***
We joined the NRC Word-Emotion Association Lexicon to our data, which allowed us to identify words associated with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).

We produce visualizations comparing the sentiments being expressed in each sample period.

Compared to our 2020 tweets, the 2021 tweets express less trust, less surprise, less joy, less disgust, less anticipation, and less anger, but more sadness, more fear, and more positivity,

Looking at the specific words underlying the 2020 and 2021 sentiments, we can see that the word "pandemic" has been most used but with a different frequency in each sample period. In 2021, other negatively valenced words such as "bad" and "sh*t" words became more common, as did positively valenced words such as "hope" and "love". This is interesting because it demonstrates that after a year, people seem to be more expressive, likely from the fallout and exhaustion of the previous year.


### How positive vs. negative are the tweets from each year?

```{r}
cat(paste0("Average AFINN scores for all words by date\n",
           "\n30 Mar 2020: ", round(twenty_mean_afinn, 3), 
           "\n30 Mar 2021: ", round(twentyone_mean_afinn, 3))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>%
  HTML() %>% div(style = "font-family: sans-serif; border: dashed;")
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center;",
  div(twenty_afinn_table, style = "width: 50%; height:600px;"),
  div(twentyone_afinn_table, style = "width: 50%; height: 600px;"),
  p(" ")
))
```

***

Next, we join the AFINN sentiment lexicon, a list of English terms manually rated for valence with an integer between -5 (negative) and +5 (positive) by Finn Årup Nielsen between 2009 and 2011. We use this lexicon to compute mean positivity scores for all words tweeted in each sample year.

The tweets from 2021 are slightly more positive, but the difference appears negligible.

In 2020, the word "support" (positively valenced) was the most frequently appearing word from the lexicon, whereas in 2021, the word "stop" (negatively valenced) appeared the most frequently. Note that "support" and "stop" are opposites. Perhaps initially, there were certain efforts people wanted to promote to mitigate effects of the pandemic. It could be that people grew exhausted of the pandemic and became more attitudinally opposed to certain phenomena than supportive of others.

### What topics/discussions are prevalent in tweets published on 30 Mar 2020?

```{r}
key <- 
  kbl(terms(tmod_twenty, 20) %>% as.data.frame(), booktabs = T, digits = 2) %>% 
  kable_styling()
freq <- 
  kbl(table(twenty_dfm$topic), booktabs = T, digits = 2) %>% kable_styling()
table(twenty_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2020 Topic Frequencies") ->
  freq_topics20
browsable(div(
  h2("2020 Topic Key", style = "font-family: sans-serif"),
  div(HTML(key), style = "width: 100%;"),
  tags$br(),
  div(ggplotly(freq_topics20), style = "width: 100%;"),
  p(" ")
))
```


***

Now, we use the quanteda package's implementation of topic modeling to identify what themes/discussions are prevalent in each year. Underlying this topic modeling implementation is Latent Dirichlet allocation (LDA), a machine learning algorithm that learns clusters of words that tend to occur together (topics). Tweets, therefore, are understood as heterogeneous mixtures of these topics. For each tweet, probabilities are assigned for each topic that the document may or may not include, and we will assume that the topic assigned the highest probability by the algorithm is the focus of the tweet.

While these topics may initially seem to make little sense, there are some patterns we can pick out.

Topic 1 seems to be the informational topic concerning outbreaks, data, hospitalizations, deaths, etc.

Topic 2, it would seem, is focused on the crisis' impact on the nation: businesses, governments, schools, and people.

Topic 3 appears to be focused on Trump. More specifically, it seems to be about media-related topics such as press briefings, live news, etc. Topic 3 was among the most frequently discussed topics, as a lot of attention was focused on the president.

Topic 4 seems to encompass emotionally intense tweets reflecting fear, anger, and hope. It includes multiple curse words along with words like "love," "hope," "kill," and "die."

Topic 5 is puzzling; there is no apparent connection between the WHO, Dr. Tedros, and Lady Gaga. We eventually found out that these words correspond to a topic that was trending on 30 Mar 2020: a phone call between WHO Director Dr. Tedros and Lady Gaga. See here: https://twitter.com/drtedros/status/1244008665251708929?lang=en

Topic 6 encompasses tweets urging social distancing.

Topic 7 is the most distinct, as it clearly focuses on the health care situation: mask and ventilator shortages, risks posed to doctors and nurses, and inadequate testing.

Topic 8 centers on China. If we piece together the words, it seems that some of the tweets likely discuss whether it is apt to blame China (note that one of the keywords is "stop"). Additional terms include "travel," "ban," and "hoax," and "lie," which altogether imply that conversations centered on China are interwoven with virus skepticism.

Topic 9 is clearly a political topic; it seems to be discussing national- and state-level policies. For instance, it includes Governor Kemp of Georgia (GA) and the hashtag #gapol, Governor Ron DeSantis of Florida (FL) and the hashtag #flapol, the South Carolina governor press and SC-related hashtags, and, at a national level, the Senate and representatives.

Topic 10 seems to focus on things people are doing at home while quarantining---watching sports in particular: "#stayhom," "#quarantinelif," "read," "play," "watch," "game," and "day." This was the second most prevalent topic. It seems that many people were tweeting about their day-to-day experience in quarantine.


### What topics/discussions are prevalent in tweets published on 30 Mar 2021?


```{r}
key <- 
  kbl(terms(tmod_twentyone, 20) %>% as.data.frame(), booktabs = T, digits = 2) %>% 
  kable_styling()
freq <- 
  kbl(table(twentyone_dfm$topic), booktabs = T, digits = 2) %>% kable_styling()
table(twentyone_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2021 Topic Frequencies") ->
  freq_topics21
browsable(div(
  h2("2020 Topic Key", style = "font-family: sans-serif"),
  div(HTML(key), style = "width: 100%;"),
  tags$br(),
  div(ggplotly(freq_topics21), style = "width: 100%;"),
  p(" ")
))
```


***

The topics from 2021 are harder to interpret.

Topic 1 mentions the Peter Navarro scandal, wherein he---a Trump advisor---allegedly personally profited from questionable/corrupt COVID-19 vaccine investment decisions. None of the other words capture a distinct theme; there are also mentions of taxes, the POTUS, China blaming, etc.

Topic 2 discusses the handling of children and families with respect to schools, travel, and jobs.

Topic 3 seems to be talking about a sports game featuring Brisbane, which people were presumably tuning into. This inference is based on the following words: "watch," "play," "brisban," "game," and "season."

We cannot make sense of topic 4.

Topic 5 focuses on the vaccine rollout.

Topic 6 seems focused on Deborah Birx's comments right around that time, when she claimed that most COVID-19 deaths could have been prevented by Trump and Fauci.

Topic 7 concerns the border crisis---the surge of illegal migrants coming to the US from Mexico, which possibly raises public health fears.

Topic 8 encourages mask wearing and social distancing.

Topic 9 seems informational---focused partly on the COVID situation in China, though the mention of "lab" makes me think that there are conspiracy theories captured by this topic.

Topic 10 is the emotional topic, with angry curse words and words like "love," "feel," "hope," and "God."

### What are the most common hashtags in each sample period?

```{r}
hashtags20 <-
  kbl(head(tstat_freq_twenty %>% as.data.frame() %>% select(1,2), 20, digits = 2), booktabs = T) %>% 
  kable_styling()
hashtags21 <-
  kbl(head(tstat_freq_twentyone %>% as.data.frame() %>% select(1,2), 20, digits = 2), booktabs = T) %>% 
  kable_styling()
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 50%; font-family: sans-serif"),
  h2("2021", style = "width: 50%; font-family: sans-serif"),
  div(HTML(hashtags20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(hashtags21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;")
))
```

***

In 2020, the most popular hashtags, of course, are #COVID19 and #coronavirus, but after these, the hot topics seem to be political topics (#gapol, #flapol, #scpolitics, #boycotttrumppressconferences) and social distancing messaging (#stayhome, #socialdistancing, #quarantinelife, #flattenthecurve).

In 2021, some of the most popular hashtags are anti-CCP, seemingly driven by the theory that China developed/released the virus intentionally. Dr. Li-Meng Yan is one of the key conveyors of this theory, as you can see on her Twitter page: https://twitter.com/DrLiMengYAN1

### How often is each country mentioned in each sample period?


```{r}
cntrymentions20 <-
  head(count20, 10) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "country") +
  ggtitle("30 Mar 2020 Country Mentions")
ggplot(dat_country20, aes(map_id = id)) +
      geom_map(aes(fill = frequency), map = world_map) +
      expand_limits(x = world_map$long, y = world_map$lat) +
      scale_fill_continuous(name = "Frequency") +
      theme_void() +
      coord_fixed() + ggtitle("30 Mar 2020: Number of Country Mentions on Twitter") ->
  cntryplot20
cntrymentions21 <-
  head(count21, 10) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "country") +
  ggtitle("30 Mar 2021 Country Mentions")
ggplot(dat_country21, aes(map_id = id)) +
      geom_map(aes(fill = frequency), map = world_map) +
      expand_limits(x = world_map$long, y = world_map$lat) +
      scale_fill_continuous(name = "Frequency") +
      theme_void() +
      coord_fixed() + ggtitle("30 Mar 2021: Number of Country Mentions on Twitter") ->
  cntryplot21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(cntrymentions20), style = "width: 50%;"),
  div(ggplotly(cntrymentions21), style = "width: 50%;"),
  div(HTML(ggplot_image(cntryplot20, height = px(500))), style = "width: 50%; justify-content: center;"),
  div(HTML(ggplot_image(cntryplot21, height = px(500))), style = "width: 50%; justify-content: center;"),
  p(' ')
))
```



***

We examine this question for both March of 2020 and 2021. We use the newsmap model as described on the quanteda package website: https://tutorials.quanteda.io/machine-learning/newsmap/

After formatting the data into country-level document feature matrices, we show the estimated number of mentions for each country.

It appears that for both datasets, the most mentions concern the United States. However, in 2020, a greater share of attention is centered on China than on the next two most mentioned locations (Britain and Canada).

We can visualize this using geographic heatmaps.

As you can see, China is brighter in the 2020 map (as is India and Australia to an extent), which indicates a higher frequency of mentions.

In 2021, the English-speaking Twitter user-base seems to be slightly more focused on its home countries than on China, though China still receives substantial attention.


### What words are associated with each country?

```{r}
formatcoef <- function(arg_1) {
  df <- 
    as.data.frame(arg_1)
  df <- 
    tibble::rownames_to_column(df, "word") %>% rename(coef = 2)
  return(kbl(df, booktabs = T, digits = 2) %>% kable_styling())
}
us20 <- formatcoef(coef(newsmap20)$US)
cn20 <- formatcoef(coef(newsmap20)$CN)
gb20 <- formatcoef(coef(newsmap20)$GB)
ca20 <- formatcoef(coef(newsmap20)$CA)
us21 <- formatcoef(coef(newsmap21)$US)
cn21 <- formatcoef(coef(newsmap21)$CN)
gb21 <- formatcoef(coef(newsmap21)$GB)
ca21 <- formatcoef(coef(newsmap21)$CA)
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", tags$br(), style = "width: 100%; font-family: sans-serif;"),
  tags$br(),
  h3("US", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("China", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("Great Britain", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("Canada", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(us20), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(cn20), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(gb20), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(ca20), tags$br(), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  tags$br(),
  h2("2021", tags$br(), style = "width: 100%; font-family: sans-serif;"),
  tags$br(),
  h3("US", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("China", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("Great Britain", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  h3("Canada", tags$br(), style = "width: 25%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(us21), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(cn21), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(gb21), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(ca21), style = "width: 25%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;")
))
```

***

For each of the two sample periods, we would like to look at what words are associated with each country. We specifically look at the four most mentioned countries in each dataset: the US, China, Great Britain, and Canada.

**2020 Interpretation**

It is difficult to make sense of these words, but there are a few whose meanings are obvious. There seems to be a lot of focus on expertism in the US with words like "scientists," "propaganda," and "expert." There's also discussion of relief bills and certain people being irresponsible.

The China conversation centers around communism, racism, and international travel---to Europe and India.

The Britain-/Canada-related words don't show any obvious themes, but it seems that the ventilator shortage in the UK was one salient topic.

**2021 Interpretation**

It is clear that the conversations surrounding these countries has changed in the past year. Rochelle Walensky, the new CDC Director is one term that sticks out. Others are the discussion of the country reopening, energy policy, and borders. We see that all of these terms are more specific than the general focus on scientists and experts that we saw in 2020. Perhaps our national fog is clearing as our country disseminates the vaccine and progress is made.

The China discussion still centers on theories about the origin of COVID-19, with mentions of a lab, bats, and Wuhan. Racism is still a common topic, especially given the recent hate crimes in the US.

THe UK mentions occur in the contexts of relations with the EU, worries about a new strain, and the Johnson & Johnson vaccine. Concerning Canada, we see talks of the AstraZeneca vaccine, which recently rolled out there. The other words listed are less easy to interpret.


IV. Analyzing Retweets {.storyboard}
=========================================

### Which are the most followed accounts being retweeted in our sample?

```{r}
library(packcircles)
mostfollowers20 <- 
  twenty_rts %>% 
  group_by(ScreenName) %>% 
  summarize(Followers = mean(Followers)) %>% 
  arrange(desc(Followers)) %>%
  head(20) %>% as.data.frame()
packing <- circleProgressiveLayout(mostfollowers20$Followers, sizetype='area')
mostfollowers20 <- cbind(mostfollowers20, packing)
dat.gg <- circleLayoutVertices(packing, npoints=50)
ggplot() + 
  # Make the bubbles
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=as.factor(id)), colour = "black", alpha = 0.6) +
  # Add text in the center of each bubble + control its size
  geom_text(data = mostfollowers20, aes(x, y, size=Followers, label = ScreenName)) +
  scale_size_continuous(range = c(1,4)) +
  # General theme:
  theme_void() + 
  theme(legend.position="none") +
  ggtitle('2020 Most Followers in Sample') +
  coord_equal() -> mostfollowers20
mostfollowers21 <- 
  twentyone_rts %>% 
  group_by(ScreenName) %>% 
  summarize(Followers = mean(Followers)) %>% 
  arrange(desc(Followers)) %>%
  head(20) %>% as.data.frame()
packing <- circleProgressiveLayout(mostfollowers21$Followers, sizetype='area')
mostfollowers21 <- cbind(mostfollowers21, packing)
dat.gg <- circleLayoutVertices(packing, npoints=50)
ggplot() + 
  # Make the bubbles
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=as.factor(id)), colour = "black", alpha = 0.6) +
  # Add text in the center of each bubble + control its size
  geom_text(data = mostfollowers21, aes(x, y, size=Followers, label = ScreenName)) +
  scale_size_continuous(range = c(1,4)) +
  # General theme:
  theme_void() + 
  theme(legend.position="none") +
  ggtitle('2021 Most Followers in Sample') +
  coord_equal() -> mostfollowers21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(mostfollowers20), style = "width: 50%;"),
  div(ggplotly(mostfollowers21), style = "width: 50%;")
))
```

***

**2020**

Obama was the most followed person getting retweeted on that day, and it seems that Katy Perry was second. India PM Narendra Modi also was getting retweeted at the time, along with several news outlets, politicians, and celebrities.

**2021**

In 2021, it seems that Obama was far and away the most followed person getting retweeted with nobody else coming near. News outlets encompass a greater share of the top 20 in follows---perhaps because there are less celebrities talking about COVID-19 in March of 2021 than in 2020.

### Whose tweets aggregately reached the highest retweet counts in each of the two sample periods?

```{r}
twenty_rts %>% 
  group_by(ScreenName) %>% 
  summarize(rt_total = sum(Retweets)) %>% 
  arrange(desc(rt_total)) %>% 
  head(20) %>% kbl(booktabs = T) %>% kable_styling() -> mostrts20
twentyone_rts %>% 
  group_by(ScreenName) %>% 
  summarize(rt_total = sum(Retweets)) %>% 
  arrange(desc(rt_total)) %>% 
  head(20) %>% kbl(booktabs = T) %>% kable_styling() -> mostrts21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 50%; font-family: sans-serif;"),
  h2("2021", style = "width: 50%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(mostrts20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(mostrts21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;")
))
```

***

For both lists, most of these names are not particularly recognizable with the exception of Joe Biden, Barack Obama, and Nicolas Maduro (2021).


### Who is the best at getting retweeted (who gets retweeted by the greatest share of followers for each tweet)?

```{r}
twenty_uqtweets <- 
  twenty_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))
twenty_rts %>% 
  group_by(ScreenName) %>%
  summarize(Followers = round(mean(Followers)), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>%
  mutate(rt_index = (Retweets/n)/Followers) %>% 
  arrange(desc(rt_index)) %>% 
  head(10) %>% 
  kbl(booktabs = T, digits = 2) %>% kable_styling() -> tweetgame20
twentyone_uqtweets <- 
  twentyone_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))
twentyone_rts %>% 
  group_by(ScreenName) %>%
  summarize(Followers = round(mean(Followers)), Retweets = sum(Retweets)) %>% 
  left_join(twentyone_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>%
  mutate(rt_index = (Retweets/n)/Followers) %>% 
  arrange(desc(rt_index)) %>% 
  head(10) %>% 
  kbl(booktabs = T, digits = 2) %>% kable_styling() -> tweetgame21
writeLines(c(paste0("30 Mar 2020 Mean RTs: ", 
                  round(mean(twenty_rts$Retweets), 2)), 
           paste0("30 Mar 2021 Mean RTs: ", 
                  round(mean(twentyone_rts$Retweets), 2)))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>% 
  HTML() -> mean_diff_rts
t.test(x = twenty_rts$Retweets, 
       y = twentyone_rts$Retweets, 
       alternative = "greater", var.equal = FALSE) %>% 
  capture.output() %>% paste(collapse = "<br>") %>% HTML() ->
  rt_difftest
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 50%; font-family: sans-serif;"),
  h2("2021", style = "width: 50%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(tweetgame20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(tweetgame21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  p(mean_diff_rts, style = "width: 100%; font-family: sans-serif; border: dashed;")
))
```


***

We compute an index for this---a proxy for "Twitter game." First, we determine the number of unique tweets for each screen name. Then, we produce a ratio of number of retweets per tweet to number of followers.

We restricting this analysis to people who have multiple original tweets getting retweeted in our samples. Perhaps this would eliminate from our top ten those users who had one tweet take off but received no attention on their others.

In 2020, camilacousseau over two tweets averaged over six retweets per follower. Nobody else exceeded 55% of this impressive total.

In 2021, richardajabu had an infinitely high retweet index by managing to get two tweets retweeted three times despite having zero followers. Incredible.

We also check whether there's a difference in mean retweets for each sample. Among the tweets that were retweeted (an important specification), the average number of retweets were *much higher* in 2020 than in 2021. An unpaired two sample t-test leads us to accept the one-sided alternative hypothesis (p<0.01) that the mean number of retweets on 30 Mar 2020 was substantially greater than the mean number of retweets on the same date in 2021. It seems that people were hitting the retweet button a lot more in the spring of 2020 than in 2021, which makes sense given that people were stuck at home in lockdown with a lot of novel happenings to discuss. Now, with vaccines being rolled out and coronavirus in decline, there is much less panic, anger, and dramatic attention being paid towards the virus.

### In each year, who was the worst at getting retweeted?

```{r}
twenty_rts %>% group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>% 
  mutate(`rt_index*1e5` = ((Retweets/n)/Followers) * 100000) %>% 
  arrange(`rt_index*1e5`) %>% head(10) %>% 
  kbl(booktabs = T, digits = 2) %>% kable_styling() -> worstrt20
twentyone_rts %>% group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>% 
  mutate(`rt_index*1e5` = ((Retweets/n)/Followers) * 100000) %>% 
  arrange(`rt_index*1e5`) %>% head(10) %>% 
  kbl(booktabs = T, digits = 2) %>% kable_styling() -> worstrt21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 50%; font-family: sans-serif;"),
  h2("2021", style = "width: 50%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(worstrt20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(worstrt21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;")
))
```

***

**2020**

ElNacionalWeb, NDTV, Reuters, and the Guardian are all terrible at getting retweeted, which makes sense because they likely tweet a lot of boring, matter-of-fact news as opposed to the clickbait-y headlines that sites like Fox or the New York Times tweet.

**2021**

Again, we see mostly news sites failing to get many retweets. There isn't anything too interesting to be said about this.


### Using our previous topic models, which topics were being retweeted about the most in each year?

```{r}
table(twenty_rts_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2020 Topic Frequencies") ->
  rt_topics20
table(twentyone_rts_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity", fill="firebrick") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2021 Topic Frequencies") ->
  rt_topics21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(rt_topics20), style = "width: 50%;"),
  div(ggplotly(rt_topics21), style = "width: 50%;")
))
```

***

**2020**

It seems that topic 9 was being retweeted about the most, which is very interesting because topic 9 had the fewest original tweets out of all topics we identified from the 2020 data.

Recall that topic 9 was about politics. Perhaps people tend to promote the views of media outlets or political influencers whose content they consume but don't have much to personally contribute to these conversations.

It may also be the case that retweeting and publishing original tweets are zero sum behaviors. In other words, when people are more likely to retweet about a particular topic, maybe that makes them less likely to also tweet about it themselves. Maybe others' have summed up their thoughts better than they can convey.

We will see if this pattern is also seen in the 2021 data.

**2021**

This hypothesized explanation is supported by the 2021 data; topic 1 is the most retweeted about but is the least originally tweeted about. Recall that topic 1 was a sort of catch-all alarmist topic, though it mentions Peter Navarro. It may be the case that retweets displace tweets about the same topic.


### Is there a difference in tweet length among retweets in each sample year?

```{r}
writeLines(c(paste0("30 Mar 2020 Mean Retweeted Tweet Length: ", 
                  mean(twenty_rts_wordcounts$tweetLength)), 
           paste0("30 Mar 2021 Mean Retweeted Tweet Length: ", 
                  mean(twentyone_rts_wordcounts$tweetLength)))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>% 
  HTML() -> rt_lengths
ggplot(data = twenty_rts_wordcounts, aes(x = tweetLength)) + geom_histogram(fill="firebrick", color = "firebrick") + ggtitle("30 Mar 2020 - Histogram of Tweet Length") -> rtlength20
ggplot(data = twentyone_rts_wordcounts, aes(x = tweetLength)) + geom_histogram(fill="firebrick", color = "firebrick") + ggtitle("30 Mar 2021 - Histogram of Tweet Length") -> rtlength21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  p(rt_lengths, tags$br(), style = "width: 100%; font-family: sans-serif; border: dashed"),
  tags$br(),
  div(ggplotly(rtlength20), style = "width: 50%;"),
  div(ggplotly(rtlength21), style = "width: 50%;")
))
```


***

We see that 2020 retweets are slightly shorter and conduct a t-test.

It seems the difference in tweet lengths is statistically significant. 2020 tweets (those being retweeted) were slightly shorter.


### Which sentiments are prevalent among retweeted tweets?

```{r}
twenty_rts_sentiment_analysis %>% 
  summarize(sum(n)) %>% 
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = `sum(n)` )) +
  geom_bar(stat = "identity", fill="firebrick") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2020 Sentiments") -> sentiments_rt_20
twentyone_rts_sentiment_analysis %>%  
  summarize(sum(n)) %>% 
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = `sum(n)` )) +
  geom_bar(stat = "identity", fill="firebrick") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2021 Sentiments") -> sentiments_rt_21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(sentiments_rt_20), style = "width: 50%;"),
  div(ggplotly(sentiments_rt_21), style = "width: 50%;"),
  p(" ")
))
```


***

The sentiments observed are not strikingly different, but there is more sadness, less surprise, more negative sentiments, and less joy being expressed in 2021---over a year from the start of the pandemic in the US.

### Is there a difference between RT AFINN scores in each period?

```{r}
cat(paste0("Average AFINN scores for all retweeted words by date\n",
           "\n30 Mar 2020: ", round(twenty_rts_mean_afinn, 3), 
           "\n30 Mar 2021: ", round(twentyone_rts_mean_afinn, 3))) %>% capture.output() %>% 
  paste(collapse = "<br>") %>% HTML() -> rt_afinns
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  p(rt_afinns, tags$br(), style = "width: 100%; font-family: sans-serif; border: dashed;"),
  div(twenty_rts_afinn_table, style = "width: 50%; height:600px;"),
  div(twentyone_rts_afinn_table, style = "width: 50%; height: 600px;"),
  p(" ")
))
```


***

The 2021 retweets appear to be more negatively valenced. 

The most common word in each period is "death." It seems that this word is much more common among retweets than among original tweets. Perhaps, the topic is too heavy for people to feel compelled to write their own tweets about it but are willing to retweet others' words. "F***" is much more common in 2020 than in 2021. Perhaps this is because people were more panicked at the start of the pandemic.


### What textual features (sentiments and words) predict how viral a tweet became in 2020?

```{r}
vip(rf_model20, num_features = 25, aesthetics = list(fill = "firebrick")) + 
  ggtitle('Random Forests - 2020 Variable Importance for Virality') -> vip_rf20
library(MASS)
twenty_vars <- importance(rf_model20) %>% as.data.frame() %>% rownames_to_column() %>% arrange(desc(.))
b <- paste(twenty_vars$rowname[1:25], collapse="+")
formula <- as.formula(paste("virality ~ ",b,sep = ""))
# Fit the full model 
full.model <- lm(formula, data = for_decisiontree_twenty %>% drop_na())
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
tab_model(step.model)$knitr %>% HTML() -> stepmodel20
vip(rf_model21, num_features = 25, aesthetics = list(fill = "firebrick")) + 
  ggtitle('Random Forests - 2021 Variable Importance for Virality') -> vip_rf21
twentyone_vars <- importance(rf_model21) %>% as.data.frame() %>% rownames_to_column() %>% arrange(desc(.))
b <- paste(twentyone_vars$rowname[1:25], collapse="+")
formula <- as.formula(paste("virality ~ ",b,sep = ""))
# Fit the full model 
full.model <- lm(formula, data = for_decisiontree_twentyone %>% drop_na())
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
tab_model(step.model)$knitr %>% HTML() -> stepmodel21
browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(vip_rf20), style = "width: 50%;"),
  div(ggplotly(vip_rf21), style = "width: 50%;"),
  div(h3('2020 Stepwise Regression'), tags$br(), p(stepmodel20, style = "font-family: sans-serif; border: dashed;"), style = "width: 50%;"),
  div(h3('2021 Stepwise Regression'), tags$br(), p(stepmodel21, style = "font-family: sans-serif; border: dashed;"), style = "width: 50%;")
))
```


***
Let's see if using sentiments, mean AFINN score, and topic-specific words, we can predict a given tweet's retweet-to-follower ratio (a measure of how viral a tweet was---to what extent it took off). We produce two models for each year---a random forests model and a stepwise regression optimized using AIC. We could include tweet length but opt not to because we are not interested in generating a highly predictive model but understanding what semantic content is being retweeted, and tweet length does not speak strongly to that question.

We are not interested in knowing the predictive efficacy or goodness of fit of the random forests model. We only want to know, out of the features we have selected, which seem the most important to predicting virality. As such, we display variable importance plots for each model and use these for interpretation.

**2020**

According to our random forests model, the most predictive variables are positive sentiments, disgust, and the words "sick," "hoax," and "country" in all of its variations.

Because the model is predicting both high and low values, the sign of each predictor (positive or negative effect on virality) is unclear. However, we would surmise that features with a strong positive effect would be the most likely to be selected given that most of the variation is positive.

However, because we can't be sure about this, we build a simple backward stepwise linear regression (optimized on AIC) to determine which effects were positive and which were likely negative.

It appears that using the word "hoax" predicts a substantial increase in virality---to the tune of over five retweets per follower. The AFINN variable has a negative coefficient, which implies that more positive tweets go less viral. Confusingly, the opposite effect is seen with the joy sentiment. Perhaps joy denotes something more specific than a positive AFINN score would imply. Surprise sentiments predict less virality, and, for whatever reason, the use of the word "gonna" predicts a substantial increase in virality.

**2021**

For our 2021 random forests model, the word "take" was the most predictive, followed by positive sentiments, trust sentiments, anger, surprise, fear and joy. It seems that particular words were much less predictive on 30 Mar 2021. Perhaps this means that there are fewer "hot topics," so mood is our best way to finger the pulse now relative to early in the pandemic.

Again, we also produce a backward stepwise regression, which confirms the predictive significance of the word "take" and shows that it predicts more retweets per follower. Positive sentiments, fear, and joy predict less virality, whereas trust predicts more.



V. Conclusion
=========================================
<div class="ourtext">

```{r, echo = FALSE}
h2("Conclusion")
```


By examining tweets from these two dates, we were able to uncover a trove of insights about how COVID-19 communications have changed—both in sentiments and in content. We see that focus on China has abated slightly. Similarly, the topics that people are discussing have changed. Conversations about lockdown are less prevalent, and political discussions have changed as well. The kinds of tweets going viral (getting heavily retweeted) differ substantially between the two years. In fact, the average number of retweets has itself shifted dramatically, with much fewer retweets overall in 2021. While the presence of particular words cann’t by itself predict how viral a tweet is, there are clear patterns about which words attracted attention during each time.

In developing a machine-learning algorithm, we can assist political leaders in crafting tweets that address the wants and needs of the populace. We can also help these leaders design their tweets in such a way that increases the likelihood that the tweets are seen by larger numbers of people.



```{r, echo = FALSE}
h2("Limitations")
```

First, we ultimately compared one day of tweets to another day of tweets one year later. While this computation already consumed a massive amount of computational power and memory, a more robust analysis might have compared a full week or even a month of tweets to another week or month of tweets. Because we only analyzed one day of tweets, there is a chance that our analysis could be capturing idiosyncratic variation for a given day rather than the general underlying sentiment of a prolonged time period.  It goes without saying, however, that a full day of tweets is certainly more robust than one hour (or even one minute) of tweets.

Second, in this analysis, only certain kinds of people are drawn to Twitter, so Twitter analysis does not necessarily offer an accurate representation of how the population feels as a whole.  More outspoken and extroverted people are more likely to express their views on Twitter, which leads to the underrepresentation of more soft spoken and introverted people. Therefore, this Twitter Analysis would be best utilized in combination with other data collection and analysis methods.

```{r, echo = FALSE}
h2("Contributions")
```

Ammar Plumber did the data retrieval and cleaning along with the topic modeling, newsmaps, and the exploratory/predictive analysis of virality.

Elaina Lin produced exploratory visualizations including word frequencies and sentiment analyses, as well as the CSS for the webpage. She also contributed to the writing.

Kim Nguyen assisted with analyses of tweet length in both original tweets and retweets. She also contributed to the writing and webpage formatting efforts.

Ryan Karbowicz produced hashtag counts, topic frequency bar graphs for each sample, and much of the writing.

Meghan Aines helped with the writing and formatting of the webpage.

</div>
